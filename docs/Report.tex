
\documentclass[11pt]{article}

\usepackage{amsmath} %import amsmath for align command
\usepackage{cite} %import package for using bibtex bibliography
\usepackage{graphicx} %import package for inserting figures from image files
\usepackage{mathtools} %import package for using certain symbols such as eq. arrows
\usepackage{tikz} %import package for creating figures
\usepackage{booktabs}
\usepackage{siunitx}
\usepackage[T1]{fontenc}
\usepackage[font=small,skip=0pt]{caption}


\usepackage{placeins}
\usepackage{array}
\newcolumntype{P}[1]{>{\centering\arraybackslash}p{#1}}

\usepackage[nodisplayskipstretch]{setspace}


\usepackage{titlesec}
\titlespacing{\section}{0pt}{0.8\baselineskip}{0.8\baselineskip}
\titlespacing{\subsection}{0pt}{0.675\baselineskip}{0.675\baselineskip}
\setlength{\abovecaptionskip}{2pt plus 2pt minus 5pt}

% for referencing links
\usepackage{hyperref}
\hypersetup{
	colorlinks=true,
	linkcolor=blue,
	filecolor=magenta,
	urlcolor=cyan,
}

% \usepackage{apacite}

\usepackage{algorithm}
\usepackage[noend]{algpseudocode}
\usepackage{textcomp}
\usepackage{subcaption}

%change default margins
\setlength{\topmargin}{-.75in}
\setlength{\textheight}{9.5in}

\setlength{\oddsidemargin}{0in}
\setlength{\evensidemargin}{0in}
\setlength{\textwidth}{6.6in}

\graphicspath{{aima/images/}}

\newcommand{\urlNewWindow}[1]{\href[pdfnewwindow=true]{#1}{\nolinkurl{#1}}}
\newcommand{\problemone}{grid world problem}
\newcommand{\Problemone}{grid world problem}
\newcommand{\problemtwo}{choice suggestion problem}
\newcommand{\Problemtwo}{choice suggestion problem}
\newcommand{\expnumber}[2]{{#1}\mathrm{e}{#2}}

\begin{document}

%create title
\title{Deep Reinforcement Learning Nanodegree\\
	   Project 1 -- Navigation Report}
\author{\vspace{-1mm}Chris Cadonic\\
chriscadonic@gmail.com}
\maketitle
\vspace{-1.5em}

\section{Introduction}

As part of the first project for the deep reinforcement learning nanodegree, I built and trained an agent that learns how to navigate through a world with yellow and blue bananas in an effort to maximize how many yellow bananas it can acquire.

\subsection{Environment Overview}

In the Unity-ML environment provided, the agent navigates through a world filled with yellow and blue bananas. As the agent moves through the world, if it comes into contact with a banana it will pick it up, receiving a reward of +1 if it picks up a yellow banana and -1 if it picks up a blue banana. Its goal is to maximize its reward for a single play session where it navigates and tries to pick up as many yellow bananas while avoiding blue bananas. The game is considered solved if it achieves an average score over 100 games of \textbf{+13}.

As the agent interacts, it receives information from the environment of the form:
\textbf{insert state info}

Interactions available to the agent include moving in one of four directions:
\begin{itemize}
	\item move forward,
	\item move backward,
	\item turn left,
	\item turn right,
\end{itemize}
with these four actions forming the action space $\mathcal{A}$. The agent solves the game by understanding what the optimal action is to take in each of the possible states that it finds itself in.

\section{Approach}

Initially I had approached this problem by first attempting to use Q-learning as
a first benchmark, but given the continuous nature of the state space
$\mathcal{S}$ this would also necessitate discretization or tile coding the
space. For this reason, I instead chose to implement a vanilla DQN (Deep
Q-network) and iteratively improve it to achieve better performance.

\subsection{DQN}

My first complete algorithm for DQN leveraged a simple multi-layer perceptron
using the following architecture:

\FloatBarrier

\begin{figure}[!ht]
    \centering
    \caption{}
    \label{fig:dqn-architecture}
\end{figure}

\FloatBarrier

Even though learning performance from the vanilla DQN was quite good (see results in
Fig. \ref{fig:dqn-performance}), I wanted to see explore options to determine if
performance could be improved.

\subsection{Improving DQN}

Since I had the least experience with implementing prioritized experience
replay, this was the first method I chose to expand on my vanilla DQN.

\subsubsection{Prioritized Experience Replay}

Using prioritized experience replay expands on the idea of sampling from the
replay buffer to provide experience tuples for the agent to learn from. In the
original DQN algorithm, this sampling is uniform across all stored experience
tuples in the replay buffer, whereas prioritized experience replay
\textit{prioritizes} experience tuples that resulted in a larger loss from
expected Q-values, capturing that these tuples may have been more impactful
during the learning process and will likely be more useful to sample more often
than lesser impactful experience tuples.

In order to expand on my initial implementation, I changed the following:

\subsubsection{Double DQN}


\subsubsection{Dueling DDQN}

\FloatBarrier

\begin{figure}[!ht]
    \centering
    \caption{}
    \label{fig:dueling-architecture}
\end{figure}

\FloatBarrier

\section{Results and Discussion}


\subsection{Learning Performance}

\FloatBarrier

\begin{figure}[!ht]
	\centering
	\caption{}
	\label{fig:dqn}
\end{figure}

\FloatBarrier


\subsection{Next Steps}





\section{Conclusion}




\bibliographystyle{unsrt}
\bibliography{sample}
% \begin{thebibliography}{9}
% \bibitem{littman}
% Leslie~Pack Kaelbling, Michael~L Littman, and Andrew~W Moore.
% \newblock Reinforcement learning: A survey.
% \newblock {\em Journal of artificial intelligence research}, 4:237--285, 1996.
% \end{thebibliography}

\end{document}
